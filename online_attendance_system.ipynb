{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating objects to detect the face\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades +'haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the data augmentation technique\n",
    "We make use of data augmentation technique to improve the quality of images and to increase the number of images in dataset. This data augmentation function returns 6 images with slight variation in all of them, thus to bring more versatality to the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from numpy import expand_dims\n",
    "def img_augment(img):\n",
    "    img_aug = []\n",
    "    datagen = ImageDataGenerator(horizontal_flip=True, rotation_range=45, \n",
    "                                 brightness_range=[0.2,1.5])\n",
    "    data = img_to_array(img)\n",
    "    # expand dimension to one sample\n",
    "    samples = expand_dims(data, 0)\n",
    "    # prepare iterator\n",
    "    it = datagen.flow(samples, batch_size=1)\n",
    "    # generate samples and plot\n",
    "    for i in range(5):\n",
    "        batch = it.next()\n",
    "        # convert to unsigned integers for viewing\n",
    "        image = batch[0].astype('uint8')\n",
    "        img_aug.append(image)\n",
    "    img_aug.append(img)\n",
    "    return img_aug\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataset by capturing 300 face images of a subject\n",
    "If you don't have a prior dataset, you can use this piece of code to create your own dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "i = 0\n",
    "#fa_img = np.zeros((0,224,224,3),dtype=np.float32)\n",
    "while True:\n",
    "    if i == 300:\n",
    "        break\n",
    "    else:\n",
    "        \n",
    "        #read each frame of the video and convert it to gray\n",
    "        ret, img = cap.read()\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Find faces in image using classifier\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.1, 5)\n",
    "\n",
    "        # Draw rectangle around the faces\n",
    "        on = []\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255,255), 2)  \n",
    "            face_img = img[y:y + h, x:x + w]\n",
    "            face_img =  cv2.resize(face_img, (224,224), interpolation = cv2.INTER_AREA)\n",
    "            #fa_img[0,:,:,:] = face_img/223\n",
    "\n",
    "            cv2.imwrite(\"C:\\\\Users\\\\rajpu\\\\Desktop\\\\dataset\\\\face_dataset\\\\sahil%i.jpg\"%i, face_img)\n",
    "            font = cv2.FONT_HERSHEY_TRIPLEX\n",
    "            cv2.putText(img,'face',(x,y-2), font, 0.8, (0,255,255)) \n",
    "            i = i+1\n",
    "        #display image\n",
    "        cv2.imshow('img',img) \n",
    "\n",
    "        #if user pressed 'q' break\n",
    "        if cv2.waitKey(1) == ord('q'): # \n",
    "            break;\n",
    "\n",
    "cap.release() #turn off camera \n",
    "cv2.destroyAllWindows() #close all windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and preprocessing images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When dataset for different subjects is stored in different directories"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def detect_face(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Find faces in image using classifier\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.5, 5)\n",
    "    \n",
    "    # For every face found \n",
    "    if len(faces)==0:\n",
    "        return (0,img)\n",
    "    for (x,y,w,h) in faces:\n",
    "        face_img = img[y:y + h, x:x + w]\n",
    "    return (1,face_img)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "IMG_SHAPE = 224\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "indice = 0\n",
    "boolean = 1\n",
    "pa = 'C:\\\\Users\\\\rajpu\\\\Desktop\\\\dataset'\n",
    "for file_name in os.listdir(pa):\n",
    "\n",
    "    if file_name == 'abhi':\n",
    "        label = 0\n",
    "        path = \"\".join((pa,\"/\",file_name))\n",
    "        for j,i in enumerate(os.listdir(path)):\n",
    "            if i.endswith(\".jpg\") or i.endswith(\".jpeg\"):\n",
    "                img = cv2.imread(path+\"/\"+i)\n",
    "                boolean,img =  detect_face(img)\n",
    "                if boolean ==0:\n",
    "                    continue\n",
    "                # Resizing the image into required shape\n",
    "                img =  cv2.resize(img, (IMG_SHAPE,IMG_SHAPE), interpolation = cv2.INTER_AREA)\n",
    "                images.append(img)\n",
    "                labels.append(label)\n",
    "                indice = indice + 1\n",
    "\n",
    "    elif file_name == 'meet':\n",
    "        label = 1\n",
    "        path = \"\".join((pa,\"/\",file_name))\n",
    "        for j,i in enumerate(os.listdir(path)):\n",
    "            if i.endswith(\".jpg\") or i.endswith(\".jpeg\"):\n",
    "                img = cv2.imread(path+\"/\"+i)\n",
    "                boolean,img =  detect_face(img)\n",
    "                if boolean ==0:\n",
    "                    continue\n",
    "                # Resizing the image into required shape\n",
    "                img =  cv2.resize(img, (IMG_SHAPE,IMG_SHAPE), interpolation = cv2.INTER_AREA)\n",
    "                images.append(img)\n",
    "                labels.append(label)\n",
    "                indice = indice + 1\n",
    "\n",
    "    elif file_name == 'paras':\n",
    "        label = 2\n",
    "        path = \"\".join((pa,\"/\",file_name))\n",
    "        for j,i in enumerate(os.listdir(path)):\n",
    "            if i.endswith(\".jpg\") or i.endswith(\".jpeg\"):\n",
    "                img = cv2.imread(path+\"/\"+i)\n",
    "           \n",
    "                boolean,img =  detect_face(img)\n",
    "                if boolean ==0:\n",
    "                    continue\n",
    "                # Resizing the image into required shape\n",
    "                img =  cv2.resize(img, (IMG_SHAPE,IMG_SHAPE), interpolation = cv2.INTER_AREA)\n",
    "                images.append(img)\n",
    "                labels.append(label)\n",
    "                indice = indice + 1\n",
    "\n",
    "    elif file_name == 'prashant':\n",
    "        label = 3\n",
    "        path = \"\".join((pa,\"/\",file_name))\n",
    "        for j,i in enumerate(os.listdir(path)):\n",
    "            if i.endswith(\".jpg\") or i.endswith(\".jpeg\"):\n",
    "                img = cv2.imread(path+\"/\"+i)\n",
    "                boolean,img =  detect_face(img)\n",
    "                if boolean ==0:\n",
    "                    continue\n",
    "                # Resizing the image into required shape\n",
    "                img =  cv2.resize(img, (IMG_SHAPE,IMG_SHAPE), interpolation = cv2.INTER_AREA)\n",
    "                images.append(img)\n",
    "                labels.append(label)\n",
    "                indice = indice + 1\n",
    "\n",
    "    elif file_name == 'sahil':\n",
    "        label = 4\n",
    "        path = \"\".join((pa,\"/\",file_name))\n",
    "        for j,i in enumerate(os.listdir(path)):\n",
    "            if i.endswith(\".jpg\") or i.endswith(\".jpeg\"):\n",
    "                img = cv2.imread(path+\"/\"+i)\n",
    "                boolean,img =  detect_face(img)\n",
    "                if boolean ==0:\n",
    "                    continue\n",
    "                # Resizing the image into required shape\n",
    "                img =  cv2.resize(img, (IMG_SHAPE,IMG_SHAPE), interpolation = cv2.INTER_AREA)\n",
    "                images.append(img)\n",
    "                labels.append(label)\n",
    "                indice = indice + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When dataset stored in single directory\n",
    "Reading images when data from all subjects is stored in single directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function defined to read image data and resize it to (224,224)\n",
    "def pre_process(file_name, path):\n",
    "    IMG_SHAPE = 224\n",
    "    if file_name.endswith(\".jpg\") or file_name.endswith(\".jpeg\"):\n",
    "        img = cv2.imread(pa + \"\\\\\"+file_name)\n",
    "        img =  cv2.resize(img, (IMG_SHAPE,IMG_SHAPE), interpolation = cv2.INTER_AREA)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading image dataset\n",
    "images = []\n",
    "labels = []\n",
    "indice = 0\n",
    "boolean = 1\n",
    "pa = 'C:\\\\Users\\\\rajpu\\\\Desktop\\\\dataset\\\\face_dataset'\n",
    "for file_name in os.listdir(pa):\n",
    "    if file_name.startswith(\"meet\"):\n",
    "        label = 0\n",
    "        img = pre_process(file_name, pa)\n",
    "    elif file_name.startswith(\"paras\"):\n",
    "        label = 1\n",
    "        img = pre_process(file_name, pa)\n",
    "    elif file_name.startswith(\"sahil\"):\n",
    "        label = 2\n",
    "        img = pre_process(file_name, pa)\n",
    "\n",
    "    images.append(img)\n",
    "    labels.append(label)\n",
    "    indice = indice + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "889it [00:46, 19.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# Calling the data augmentation function\n",
    "img = []\n",
    "X = []\n",
    "y = []\n",
    "for j,image in tqdm(enumerate(images)):\n",
    "    img = img_augment(image)\n",
    "    for i in img:\n",
    "        X.append(i)\n",
    "        y.append(labels[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping images\n",
    "IMG_SHAPE = 224\n",
    "n=len(X)\n",
    "\n",
    "data_images = np.zeros((n,IMG_SHAPE,IMG_SHAPE,3),dtype=np.float32)\n",
    "data_labels = np.zeros((n),dtype=np.float32)\n",
    "\n",
    "i = 0\n",
    "for item in range(len(X)):\n",
    "    data_images[i,:,:,:] = X[item]/223\n",
    "    data_labels[i] = y[item]\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and testing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test, y_train, y_test = train_test_split(data_images, data_labels, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.layers import Dense, Flatten, BatchNormalization, Activation, UpSampling2D\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.constraints import maxnorm\n",
    "from keras.utils import np_utils\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 222, 222, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 111, 111, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 109, 109, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 54, 54, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 52, 52, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 26, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 64)        73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 10, 10, 32)        18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                51264     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 237,093\n",
      "Trainable params: 237,093\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(5))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = keras.optimizers.Adam(lr=0.001),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "768/768 [==============================] - 117s 150ms/step - loss: 0.6165 - accuracy: 0.7278 - val_loss: 0.1734 - val_accuracy: 0.9479\n",
      "Epoch 2/10\n",
      "768/768 [==============================] - 116s 152ms/step - loss: 0.1317 - accuracy: 0.9544 - val_loss: 0.0541 - val_accuracy: 0.9833\n",
      "Epoch 3/10\n",
      "768/768 [==============================] - 117s 153ms/step - loss: 0.0519 - accuracy: 0.9833 - val_loss: 0.0438 - val_accuracy: 0.9844\n",
      "Epoch 4/10\n",
      "768/768 [==============================] - 112s 146ms/step - loss: 0.0443 - accuracy: 0.9846 - val_loss: 0.0494 - val_accuracy: 0.9833\n",
      "Epoch 5/10\n",
      "768/768 [==============================] - 112s 146ms/step - loss: 0.0209 - accuracy: 0.9925 - val_loss: 0.0255 - val_accuracy: 0.9906\n",
      "Epoch 6/10\n",
      "768/768 [==============================] - 115s 149ms/step - loss: 0.0218 - accuracy: 0.9924 - val_loss: 0.0261 - val_accuracy: 0.9917\n",
      "Epoch 7/10\n",
      "768/768 [==============================] - 121s 157ms/step - loss: 0.0138 - accuracy: 0.9955 - val_loss: 0.0207 - val_accuracy: 0.9927\n",
      "Epoch 8/10\n",
      "768/768 [==============================] - 122s 159ms/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 0.0971 - val_accuracy: 0.9750\n",
      "Epoch 9/10\n",
      "768/768 [==============================] - 123s 161ms/step - loss: 0.1081 - accuracy: 0.9748 - val_loss: 0.0136 - val_accuracy: 0.9927\n",
      "Epoch 10/10\n",
      "768/768 [==============================] - 125s 163ms/step - loss: 0.0071 - accuracy: 0.9986 - val_loss: 0.0130 - val_accuracy: 0.9937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18091288ec8>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=5, epochs=10, validation_split=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"C:\\\\Users\\\\rajpu\\\\Desktop\\\\dataset\\\\models\\\\model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test, batch_size = 5 )\n",
    "y_pred = list(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -9.326103,  -4.02574 ,  26.434675, -32.273632, -40.12623 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_output = []\n",
    "for i in range(len(y_pred)):\n",
    "    max_val = max(y_pred[i])\n",
    "    max_val_ind = list(y_pred[i]).index(max_val)\n",
    "    y_output.append(max_val_ind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multiclass and continuous-multioutput targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-d24fde5f9467>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m# compute confusion matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mcnf_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_printoptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda-Navigator\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m     \"\"\"\n\u001b[1;32m--> 268\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multiclass\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not supported\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda-Navigator\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[1;32m---> 90\u001b[1;33m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of multiclass and continuous-multioutput targets"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "class_names = ['Meet', 'Paras', 'Sahil']\n",
    "def plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "# compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, title='Normalized confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
